\documentclass[11pt]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{breakurl} 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{mathptmx}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{wrapfig}
%\usepackage{minipage}
\def \hfillx {\hspace*{-\textwidth} \hfill}

\title{Evading the Machine Learning Detector: A Virusâ€™ Perspective}
\author{Group 10 \\ Yuan Xiao, Xiaokuan Zhang}
\date{04/27/2017}



\begin{document}

\maketitle
\section*{Abstract}
% In this project, we use different classification algorithms to classify hand-written digits and pixel-displayed letters. The algorithms we use are Naive Bayes, Support Vector Machine and Artificial Neural Networks. For each algorithm, we tune different parameters in order to achieve a better performance. We use accuracy and F-measure to evaluate the performance. Generally speaking, SVM performs best among the three, and Naive Bayes does not work well. 


\section{Introduction}

\subsection{Problem Description}
There are a bunch of anti-virus software on market nowadays, such as Norton and McAfee. They can detect malicious apps/binaries and prevent them from jeopardizing the user's data. However, in our project, we are not aiming at improving the detection rate of anti-virus software; instead, we want to study from a virus' perspective: how should a virus disguise itself to evade the detection? To answer this question, we designed a novel evasion scheme and conducted experiments the test its efficiency on a dataset from UCI Machine Learning Repository. We will introduce them in more detail in Section \ref{sec:exp}.

\subsection{Classification Algorithms}
In this report, we use LibSVM \cite{CC01a} as our machine learning  algorithm, which implements Support Vector Machine (SVM). To compare with our feature selection scheme, we use Weka's \texttt{weka.attributeSelection.CfsSubsetEval} module as the feature selection method. It implements correlation-based feature selection. We will present the detail of algorithms in Section \ref{sec:metho}.

\subsection{Assumptions} \label{sec:assumption}
Known Model, Known Training set

\subsection{Originality}

\subsection{Result Summary}
Our results show that our scheme works well and can truly fool the detector.
% For the hand-written digit dataset, after tuning the parameters, the three classifiers can achieve 98.97\%, 93.27\%, 92.02\%  accuracy. For the letter dataset, they can reach 95.33\%, 82.77\%, 74.18\% accuracy. We only submit a \texttt{pdf} file and a \texttt{tex} file to Carmen. For a more detailed view of our repository, please visit our repo on github \footnote{https://github.com/XiaokuanOSU/AI2report}.

\section{Background}
The author of the dataset we used generated the dataset based on the n-gram feature set construction from Masud et al.'s paper \cite{masud2007hybrid}. In this paper, they proposed a new way to construct features for binary files, called N-gram Features. An n-gram may be either a sequence of n bytes or n assembly instructions, or n DLL function calls, depending on whether they want to extract features from binary or assembly program, or DLL call list. For example, the 3-grams corresponding to the 4 bytes sequence "a1b2c3d4" are "a1b2c3" and "b2c3d4". They constructed Binary/Assembly/DLL N-gram feature set, and proposed a hybrid model to combine the three feature sets. Then, they choose best 500 features for each set based on entropy gain. Later on, they use LibSVM to test their models. The hybrid model works best for both datasets (Accuracy:96.30\%, 96.15\%).

There is an older paper about detecting malicious executables by Schultz et al. \cite{schultz2001data}. In their paper, they constructed DLLs, GNU Strings, and Byte Sequence features. The DLL and Byte Sequence features are similar to the way of constructing our dataset, but they did not use N-gram feature. They further tested their features using different machine learning algorithms, i.e., RIPPER, Naive Bayes, and Multi-Naive Bayes. The best one (Multi-Naive Bayes) can achieve 97.76\% detection rate.


% \begin{figure}[htbp]
% \centering

% \begin{subfigure}[htbp]{0.32\columnwidth}
% \includegraphics*[width=\textwidth]{fig/ex_digit}
% \caption{A Digit Example: 2}
% \label{fig:ex:digit}
% \end{subfigure}
% \hfill
% \begin{subfigure}[htbp]{0.32\columnwidth}
% \includegraphics*[width=\textwidth]{fig/ex_letter}
% \caption{Letter Examples}
% \label{fig:ex:letter}
% \end{subfigure}
% \hfill
% \begin{subfigure}[htbp]{0.32\columnwidth}
% \includegraphics*[width=\textwidth]{fig/feature_letter}
% \caption{Features of Letters}
% \label{fig:feature:letter}
% \end{subfigure}
% \caption{Examples and Features}
% \end{figure}

\section{Methodology}\label{sec:metho}

\subsection{Tools}
\subsubsection{LibSVM}
LibSVM \cite{CC01a} is an open-source software that implements Support Vector Machine (SVM) algorithm \cite{cortes1995support}. It is one of the most popular tools when it comes to SVM implementation. It is the main tool that we use to test the performance our evasion scheme. 

\subsubsection{Weka}
Weka \cite{hall2009weka} is a collection of machine learning algorithms for data mining tasks. It has a nice GUI interface and can accept different file inputs (e.g, .arff, .csv, etc.). In our project, our main task is to find important features to add or delete (feature selection). Therefore, we use Weka as a tool for feature selection and compare the performance with our feature selection scheme.

\subsection{Algorithms}
\subsubsection{SVM}
SVM \cite{cortes1995support} is a supervised learning model that analyzes data for classification or regression. Provided with a set of training examples, which are marked with their belonging categories, a SVM algorithm performs to build a model so as to recognize and assign testing examples to the predicted categories. 

\subsubsection{Correlation-based Feature Selection}
We choose the \texttt{weka.attributeSelection.CfsSubsetEval} module as the feature selection method to compare with our scheme. It implements correlation-based feature selection, which evaluates the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them \cite{hall1999correlation}.

\subsubsection{Relative Ratio Feature Selection}
In this project, we came up with a novel way to select features to modify, which is specific to this project only. We will propose our method in detail in Section \ref{sec:exp}.
% Besides linear classification, SVMs are also able to efficiently perform non-linear classification when kernel tricks are applied\cite{boser1992training} , which will map original input into high-dimensional attribute space.

% Neural networks, or artificial neural networks \cite{hagan1996neural}, simulate the functions of nerve cells of human brain and serve as an important computational approach in machine learning. They typically form a structure of multiple layers of basic perceptrons and support both supervised and unsupervised learning.

% Neural networks have a long history, dating back to the 1940s \cite{mcculloch1943logical}. However, the idea of artificial neural networks was not popular at early days due to its limitation in solving logical calculations \cite{minsky1988perceptrons}. Modern neural networks revived in the past decade, along with the rise of deep learning \cite{bengio2009learning, schmidhuber2015deep}.

% \subsection{Naive Bayes}
% Naive Bayes classifier makes use of the Bayes Theorem. It is basically a conditional probability model. It is one of the simplest machine learning algorithms. Compared to Bayesian Networks, Naive Bayes is technically a special case by assuming that all features are conditionally independent from each other given the class label. One of the earliest papers that described this algorithm was from 1970s \cite{duda1973pattern}. 

\section{Experiments}\label{sec:exp}
% The whole experiment is made up of four steps. First, data preprocessing. Second, model training with LibSVM and testing with unmodified test set. Thus we have a criterion for evaluation. Third, data modification in the test set and testing using trained libSVM model. Fourth, performance comparison. The methods for model training and data modification are already described in section \ref{sec:metho} and the rest parts will be discussed in the this section.


\subsection{Original Dataset}
The dataset includes instances of malicious executables (computer virus) as well as non-malicious executables (normal programs). The features are extracted from real-world malicious and non-malicious executables. The dataset was published in March 2016 in UCI Machine Learning Repository. Within one year, the web hits of the dataset has already reached over 200,000. It is obvious that the dataset arouses great interest in the machine learning community.

The dataset consists of 373 instances, of which 301 are malicious and 72 are benign. Each instance has 500 hex features and 30 DLL features. Examples of hex features and DLL features are shown in Fig. \ref{fig:hexDLL}. Notice that on the UCI webpage it claims that there are 13 DLL features but we found from the raw data that there are actually 30 of them. All the attributes are binary, meaning a certain feature exists or not. 

However, the primary weakness of the dataset is that the sample number is relatively low. However, the goal of our project is not to exhaustively find the best machine learning model to classify malicious and non-malicious executables. Thus we think the low sample amount is tolerable. In addition, after searching online, the UCI dataset is the only dataset available that do not require extracting features by ourselves.



\begin{figure}[htbp]
\centering
\begin{subfigure}[htbp]{0.4\columnwidth}
\includegraphics*[width=\textwidth]{fig/hex}
\caption{Hex Feature}
\label{fig:hex}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.57\columnwidth}
\includegraphics*[width=\textwidth]{fig/DLL}
\caption{DLL Feature}
\label{fig:DLL}
\end{subfigure}
\label{fig:hexDLL}
\end{figure}

% \hfill
% \begin{subfigure}[htbp]{0.46\columnwidth}
% \includegraphics*[width=\textwidth]{fig/smo_libsvm}
% \caption{Comparison between SMO and LibSVM}
% \label{fig:smo-libsvm}
% \end{subfigure}
% \caption{Performance of Support Vector Machine}
% \label{fig:svm}
% \end{figure}

% In addition, we randomly seperate the dataset into two sets. One consists of 80\% of the dataset as the training set while the rest as the test set.
% \subsubsection{Support Vector Machine}

% As shown in Figure \ref{fig:digit-svm}, SVM classifiers provide a good performance on the Digit dataset. In the experiment, two key parameters are tuned. One is the kernel type and the other is the complexity parameter. For kernels, the linear kernel and the non-linear polynomial kernel are tested. When the polynomial kernel is applied, 98.97\% accuracy can be achieved, and 97.92\% accuracy is achieved in cases of the linear kernel. 

% From the figure, two results can be observed. Firstly, polynomial kernel has better performance than the linear one, regardless of values of the complexity parameter. Secondly, the complexity parameter has little effect on the SVM model in regards to the Digit dataset. 

% \subsubsection{Neural Networks}
% In order to understand how the many parameters influence the performance of multilayer perceptron in classification, we decide to change only one parameter at a time. The default setting is ``-L 0.3 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H a". And the parameters that we are interested in are learning rate, training time and validation threshold.

% Figure \ref{fig:digit-ann} shows the average performance for different parameter combinations. For default settings, we have the average accuracy of 92.73\%. And the root relative squared error is as low as 38.53\%. Changing validation threshold to either more or less does not affect the performance at all. And if we let training time be less, the overall accuracy lowers a little, which matches our expectation.

% It can be easily noticed that changing learning rate to 0.6 affects the performance heavily. Thus we tested with the same parameter combination again. In Figure \ref{fig:digit-ann-special}, when we ran the test again, the accuracy grew to 98.36\%. Such inconsistency in the test result showed that the performance of Multipayer Perceptron is influenced by stochastic. If we increase the stochastic by modifying the parameters, the fluctuation grows.

% \begin{figure}[htbp]
% \centering

% \begin{subfigure}[htbp]{0.46\columnwidth}
% \includegraphics*[width=\textwidth]{fig/digit_ANN}
% \caption{Digit Classification}
% \label{fig:digit-ann}
% \end{subfigure}
% \hfill
% \begin{subfigure}[htbp]{0.46\columnwidth}
% \includegraphics*[width=\textwidth]{fig/digit_ANN_special}
% \caption{L=0.6 in Digit Classification}
% \label{fig:digit-ann-special}
% \end{subfigure}
% \hfill
% \begin{subfigure}[htbp]{0.46\columnwidth}
% \includegraphics*[width=\textwidth]{fig/letter_ANN}
% \caption{Letter Recognition}
% \label{fig:letter-ann}
% \end{subfigure}
% \caption{Performance of Multilayer Perceptron}
% \label{fig:ann}
% \end{figure}

% \subsubsection{Naive Bayes}
% The Naive Bayes works well on the Hand-written Digit dataset. In Figure \ref{fig:bayes}, K means whether to use kernel estimator, and D means whether to use supervised discretization. In Figure \ref{fig:digit-bayes}, when K=F, D=F, which is the worst case scenario, the accuracy and F-measure are still higher than 90\%. In the best case (K=T, D=F), they both exceeds 92\%.


\subsection{Work Flow}
The whole experiment is made up of four steps. %First, data preprocessing. Second, model training with LibSVM and testing with unmodified test set. Thus we have a criterion for evaluation. Third, data modification in the test set and testing using trained libSVM model. We need to determine which features we want to add or delete for malicious samples, which is the key of our experiment. Fourth, performance comparison. 
We will go through each step in detail.

\subsubsection{Data Preprocessing}
The raw data is generally LibSVM-format conformant. The class attribute at the beginning of an instance marks whether it is benign or malicious. -1 stands for a malicious instance and +1 means benign. However, at the end of each instance there is an additional -1. We need to remove it before passing it to LibSVM as input.

\subsubsection{Model Training \& Testing}
After data preprocessing, we pass the training set to LibSVM to train a classification model. In \ref{sec:assumption}, we mentioned that the model is known to the attacker, and it will not change when the test set changes. %With the different cores, we will have different models. The following implementations apply to all of the models.

Given a trained classification model, we apply it to the test set first and get the test results as a criterion for later evaluation. With the unmodified test set, we expect the performance of the model to classify malicious instances to be high enough. Otherwise, the model itself does not make sense and it is pointless to fool an inaccurate classification model.


\subsubsection{Attribute Modification}
Now that we have a model, we can apply different attribute modification methods to the test set. Notice that only malicious instances are modified since our only goal is to disguise a malicious executable as a benign one. More specifically, we use different feature selection methods to find the right attributes to modify (add or delete attributes). With different modified test sets, we re-do the testing phase with unmodified classification model. 

\subsubsection{Performance Evaluation}
In the end, we compare the new test result with the original one and see whether the detection rate drops. Moreover, we compare the escape results of different modification methods to find which one performs best. 

\subsection{Relative Ratio Feature Selection}
At first, we chose to modify only one attribute. We modified each of the 530 attribute (add or delete), but none of them affected the performance of the original model. We cannot simply choose any two of them, because there are too many combinations. To solve this problem, we came up with a novel way to perform the feature selection, and we call it Relative Ratio feature selection. In this part, we will introduce this method in detail.

For every attribute X (1~530) in the training set, first, we compute the \texttt{Positive Ratio (PR)} using the following equation:

\begin{equation}
PR(X) = \frac{\text{benign instances that have attribute X}}{\text{total benign instances}}
\end{equation}

Similarly, we also compute the Nagative Ratio (NR) using the following equation:

\begin{equation}
NR(X) = \frac{\text{malicious instances that have attribute X}}{\text{total malicious instances}}
\end{equation}

Then, the \texttt{Relative Ratio (RR)} is defined as follows:

\begin{equation}
RR(X) = PR(X) - NR(X)
\end{equation}

Here, if an attribute X has high \texttt{RR}, it means that attribute X appears more often in benign samples; On the other hand, if an attribute X has low \texttt{RR}, it means that attribute X appears more often in malicious samples.

After calculating \texttt{RR} for all 530 attributes, we sort them in descending order and obtain an attribute list, \texttt{RRList}. In \texttt{RRList}, if attribute X appears earlier than Y, it means that X has larger \texttt{RR} values than Y. Then, we can perform two kinds of modification strategies:
\begin{enumerate}
\item {Adding attributes.} 
We can add top N attributes in \texttt{RRList} to the malicious samples. By adding N attributes that have high \texttt{RRs}, we make the viruses become more like benign samples.
\item {Removing attributes.}
We can remove last N attributes in \texttt{RRList} to the malicious samples, should they have such attributes. By removing N attributes that have low \texttt{RRs}, we make the viruses become less like malicious samples.
\end{enumerate}

% \begin{wraptable}[13]{r}{0.5\textwidth}
% \centering
% \begin{tabular}{c  c  c} \hline
% % centering table
% % creating 10 columns
% % inserting double-line 

% Parameters & Class (Letter) & Accuracy \\\hline
% \multirow{3}{*}{K = linear, C = 1} & S & 68\% \\
% 	& H & 69.8\% \\
% 	& R & 73.8\% \\\hline
% \multirow{3}{*}{K = poly, C = 1} & H & 91\% \\
% 	& R & 91\% \\
% 	& S & 64\% \\\hline
% \multirow{3}{*}{K = linear, C = 2} & S & 67.5\% \\
% 	& H & 69.7\% \\
% 	& B & 92.4\% \\\hline
% \multirow{3}{*}{K = poly, C = 2} & R & 90.8\% \\
% 	& H & 91.1\% \\
% 	& F & 92.5\% \\\hline
% \end{tabular}
% \caption{Worst Cases: Support Vector Machine} % title name of the table
% \label{tbl:svm}
% \end{wraptable}
%\vspace{10cm}

% \newpage

% \subsubsection{Naive Bayes}
% Naive Bayes does not work well on the Letter Recognition dataset. From Figure \ref{fig:letter-bayes}, we can see that the highest accuracy is lower than 75\%. Generally speaking, Naive Bayes is not suitable for classifying letters.

% In this dataset, we are also interested in which letters Naive Bayes performs worst. Table \ref{tbl:bayes} shows the top 3 worst cases when using Naive Bayes with different parameters. In all three parameter settings, `H' is always one of the top 3, which means that `H' is quite hard to classify for Naive Bayes. Also, the same applies to `S' and `X', as they appear in two of the three cases.

\subsection{Relative Ratio Evaluation}
\subsubsection{Adding Attributes}
\subsubsection{Removing Attributes}

\subsection{Comparison with Correlation-based Feature Selection}


\section{Discussion}

\subsection{Real-world Feasibility}
In this section we briefly discusses the feasibility of our modification towards the malicious executables and compare them with existing anti-antivirus methods deployed by real-world virus.

In reality, it is much more difficult to remove features than to add features. The features to remove mostly denote the malicious behavior of a virus and are thus hard to take out. In contrast, we can easily add features as dummy code that will never be executed.

Our modification schemes in fact serves as a combination of two existing anti-antivirus methods. The first is to disguise as popular file formats such as .pdf or .docx or programs such as calc.exe or notepad.exe. The second is polymorphic virus. It mutates on each copy by adding different types of NOP instructions.

\subsection{Defense Mechanisms}

\section{Conclusion}

% \subsection{Evaluation Matrix}
% In our presentation, we used \texttt{root relative squared error} to evaluate the performance of our algorithms and falsely claimed that SVM was not suitable for our datasets. However, it makes little sense to evaluate this feature on  non-binary datasets. So in the report, we changed it to F-measure.

% \subsection{Different Libraries of SVM}

% In our presentation, we only discussed the performance of the polynomial kernel in SMO. To have a comprehensive understanding on the performance of SVMs, experiments based on LibSVM are performed.

% Figure \ref{fig:smo-libsvm} compares the accuracy and F-measure between SMO and LibSVM when the polynomial kernel is applied. Surprisingly, although both the complexity and kernel parameters are configured as the same, the accuracy from LibSVM is higher than that from SMO by 12\%. 

% \begin{figure}[htbp]
% \centering

% \begin{subfigure}[htbp]{0.46\columnwidth}
% \includegraphics*[width=\textwidth]{fig/digit_bayes}
% \caption{Digit Classification}
% \label{fig:digit-bayes}
% \end{subfigure}
% \hfill
% \begin{subfigure}[htbp]{0.46\columnwidth}
% \includegraphics*[width=\textwidth]{fig/letter_bayes}
% \caption{Letter Recognition}
% \label{fig:letter-bayes}
% \end{subfigure}
% \caption{Performance of Naive Bayes}
% \label{fig:bayes}
% \end{figure}

% \subsection{Parameters of Multilayer Perceptron}
% Multilayer Perceptron has a number of parameters. In the project, we would like to look into the following specific parameters: learning rate, training time and validation threshold. Other parameters include momentum, seed, nominalToBinaryFilter, hiddenLayers etc.

% Learning rate (-L) stands for the amount the weights are updated. Training time (-N) is the number of epochs to train through. If the validation set is non-zero then it can terminate the network early. Validation threshold (-E) is used to terminate validation testing. The value here dictates how many times in a row the validation set error can get worse before training is terminated.

% In our tests, the validation threshold does not affect the performance.
 
%  \begin{table}[!htb]
% %\begin{subtable}

% \centering
% \begin{minipage}{0.46\columnwidth}
% \begin{tabular}{c  c  c} \hline
% % centering table
% % creating 10 columns
% % inserting double-line 

% Parameters & Class (Letters) & Accuracy \\\hline
% \multirow{3}{*}{Default} & G & 69\% \\
% 	& H & 64.4\% \\
% 	& S & 65\% \\\hline
% \multirow{3}{*}{L = 0.6} & G & 68.2\% \\
% 	& H & 69.1\% \\
% 	& S & 64\% \\\hline
% \multirow{3}{*}{N = 200} & G & 69.3\% \\
% 	& H & 65.1\% \\
% 	& S & 64.4\% \\\hline
% \end{tabular}
% \caption{Worst Cases: Multilayer Perceptron} % title name of the table
% \label{tbl:ann}
% \end{minipage}
% \hfill
% \begin{minipage}{0.46\columnwidth}
% \begin{tabular}{c  c  c} \hline
% % centering table
% % creating 10 columns
% % inserting double-line 

% Parameters & Class (Letters) & Accuracy \\\hline
% \multirow{3}{*}{K = F, D = F} & S & 29.4\% \\
% 	& H & 30.5\% \\
% 	& Y & 33.1\% \\\hline
% \multirow{3}{*}{K = T, D = F} & H & 57.4\% \\
% 	& S & 64.2\% \\
% 	& X & 64.3\% \\\hline
% \multirow{3}{*}{K = F, D = T} & H & 57.5\% \\
% 	& E & 60.7\% \\
% 	& X & 64.4\% \\\hline
% \end{tabular}
% \caption{Worst Cases: Naive Bayes} % title name of the table
% \label{tbl:bayes}
% \end{minipage}
% %\end{subtable} 
% \end{table}


\newpage
\bibliographystyle{plain}
\bibliography{Group10}
\end{document}







